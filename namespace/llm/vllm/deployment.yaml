apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-engine
  namespace: llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-engine
  template:
    metadata:
      labels:
        app: vllm-engine
    spec:
      containers:
        - name: vllm
          image: docker.io/intel/vllm:0.9.0-xpu
          imagePullPolicy: IfNotPresent
          args:
            - "--model"
            - "unsloth/Qwen3-0.6B"
            - "--device"
            - "xpu"
            - "--enable-auto-tool-choice"
            - "--tool-call-parser"
            - "hermes"
          env:
            - name: SYCL_DEVICE_FILTER
              value: "level_zero:gpu"
            - name: ZES_ENABLE_SYSMAN
              value: "1"
            - name: VLLM_PORT
              value: "8000"
            - name: VLLM_LOGGING_LEVEL
              value: "INFO"
            - name: VLLM_GPU_MEMORY_UTILIZATION
              value: "0.90"
          ports:
            - containerPort: 8000
              protocol: TCP
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            runAsGroup: 1000
            allowPrivilegeEscalation: false
            seccompProfile:
              type: RuntimeDefault
          resources:
            requests:
              cpu: "2"
              memory: "4Gi"
              gpu.intel.com/i915: "1"
            limits:
              cpu: "4"
              memory: "8Gi"
              gpu.intel.com/i915: "1"
          volumeMounts:
            - name: models
              mountPath: /models
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: vllm-models
        - name: tmp
          emptyDir: {}