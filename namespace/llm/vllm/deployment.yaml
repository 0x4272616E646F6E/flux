apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm
  namespace: llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm
  template:
    metadata:
      labels:
        app: vllm
    spec:
      initContainers:
        - name: model-downloader
          image: alpine:edge
          imagePullPolicy: Always
          command:
          - sh
          - -c
          - |
            set -e
            apk add --no-cache curl ca-certificates
            echo "Checking if model exists..."
            if [ ! -f /models/qwen3-0.6b.gguf ]; then
              echo "Downloading model..."
              for i in 1 2 3; do
                curl -fL --retry 3 --retry-all-errors -o /models/qwen3-0.6b.gguf \
                  "https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q4_K_M.gguf?download=true" && break
                echo "Retry $i failed, retrying in 5s..."; sleep 5
              done
              [ -s /models/qwen3-0.6b.gguf ] || { echo "Download failed"; exit 1; }
            else
              echo "Model already exists, skipping download."
            fi
          volumeMounts:
            - name: models
              mountPath: /models
      containers:
        - name: vllm
          image: docker.io/intel/vllm:latest
          imagePullPolicy: IfNotPresent
          command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
          args:
          - "--model"
          - "/models/qwen3-0.6b.gguf"
          - "--load-format"
          - "gguf"
          - "--device"
          - "xpu"
          - "--host"
          - "0.0.0.0"
          - "--port"
          - "3000"
          - "--max-model-len"
          - "4096"
          - "--max-num-batched-tokens"
          - "512"
          - "--gpu-memory-utilization"
          - "0.9"
          - "--rope-scaling"
          - '{"type":"yarn","factor":3.2}'
          ports:
            - containerPort: 3000
              protocol: TCP
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            runAsGroup: 1000
            allowPrivilegeEscalation: false
            seccompProfile:
              type: RuntimeDefault
          env:
          - name: SYCL_DEVICE_FILTER
            value: "level_zero:gpu"
          - name: ZES_ENABLE_SYSMAN
            value: "1"
          volumeMounts:
            - name: models
              mountPath: /models
          resources:
            requests:
              cpu: "2"
              memory: "4Gi"
            limits:
              cpu: "4"
              memory: "8Gi"
              gpu.intel.com/i915: "1"
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: vllm-models