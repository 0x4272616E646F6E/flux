apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm
  namespace: llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm
  template:
    metadata:
      labels:
        app: vllm
    spec:
      containers:
        - name: vllm
          image: docker.io/intel/vllm:latest
          imagePullPolicy: IfNotPresent
          command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
          args:
          - "--model"
          - "unsloth/Qwen3-0.6B"
          - "--host"
          - "0.0.0.0"
          - "--port"
          - "8000"
          - "--enable-auto-tool-choice"
          - "--tool-call-parser"
          - "hermes"
          - "--rope-scaling"
          - '{"rope_type":"yarn","factor":3.2,"original_max_position_embeddings":40960}'
          - "--max-model-len"
          - "131072"
          - "--gpu-memory-utilization"
          - "0.9"
          - "--enable-reasoning"
          - "--reasoning-parser"
          - "deepseek_r1"
          ports:
            - containerPort: 8000
              protocol: TCP
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            runAsGroup: 1000
            allowPrivilegeEscalation: false
            seccompProfile:
              type: RuntimeDefault
          env:
          - name: SYCL_DEVICE_FILTER
            value: "level_zero:gpu"
          - name: ZES_ENABLE_SYSMAN
            value: "1"
          - name: VLLM_PORT
            value: "8000"
          volumeMounts:
            - name: models
              mountPath: /models
            - name: tmp
              mountPath: /tmp
          resources:
            requests:
              cpu: "2"
              memory: "4Gi"
            limits:
              cpu: "4"
              memory: "8Gi"
              gpu.intel.com/i915: "1"
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: vllm-models
        - name: tmp
          emptyDir: {}